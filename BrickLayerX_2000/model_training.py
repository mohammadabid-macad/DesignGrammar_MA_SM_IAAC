# -*- coding: utf-8 -*-
"""Model_Training.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13u6Pnhf9rHtp8ODbT3ZL9SoRshSJ_hdR
"""

from google.colab import drive
import pickle
import os

# Mount the Google Drive
drive.mount('/content/drive')

# Define the base path where the saved data is stored
base_data_path = "/content/drive/MyDrive/Colab Notebooks/Thesis/BrickLayerX_2000"

#!pip uninstall -y torch torchaudio torchvision dgl

# Install PyTorch 1.13.1 and related libraries (CPU version)
!pip install torch==1.13.1 torchvision==0.14.1 torchaudio==0.13.1 --index-url https://download.pytorch.org/whl/cpu

# Install DGL 1.1.3 (CPU version, compatible with PyTorch 1.13.1)
!pip install dgl==1.1.3

"""# Step 1: Loading Data

## 1.1 Loading DGL Graphs and Features
"""

import pickle
import os

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Define the path for DGL graphs
dgl_graphs_path = os.path.join(base_data_path, 'DGLGraphs/')

# Load patterns dataset (metadata like iteration ID)
with open(os.path.join(base_data_path, 'patterns_dataset.pkl'), 'rb') as f:
    patterns_dataset = pickle.load(f)

# Load all pattern DGL graphs from the subfolder
with open(os.path.join(dgl_graphs_path, 'all_pattern_dgl_graphs.pkl'), 'rb') as f:
    all_pattern_dgl_graphs = pickle.load(f)

# Load the empty grid DGL graph from the base folder
with open(os.path.join(base_data_path, 'empty_grid_dgl_graph.pkl'), 'rb') as f:
    empty_grid_dgl_graph = pickle.load(f)

# Load the combined normalized features (from step 5.3)
with open(os.path.join(base_data_path, 'combined_normalized_features.pkl'), 'rb') as f:
    combined_normalized_features = pickle.load(f)

# Extract node, edge, subgraph, and graph features
combined_node_features = combined_normalized_features['node_features']
combined_edge_features = combined_normalized_features['edge_features']
combined_subgraph_features = combined_normalized_features['subgraph_features']
combined_graph_features = combined_normalized_features['graph_features']

print("Data loading complete.")

"""# Step 2: Embedding Features

Here's the outline for Step 2:

    Embed Node-Level Features: We will embed features such as brick type, position (x, y), orientation, occupied status, insertion point, filler module, and eigenvector centrality.

    Edge-Level Features: These features include edge connectivity and orientation match, which will be used during the neighborhood aggregation in the model.

    Graph-Level and Subgraph-Level Features: These will be handled later during pooling.

## 2.1 Embedding Node and Edge Features into the Pattern Graphs
"""

import torch

# Function to embed node-level features into the DGL graph
def embed_node_features_in_dgl_graph(dgl_graph, node_features_df):
    """Embed node-level features from the dataframe into the DGL graph."""
    x_pos = torch.tensor(node_features_df['x_pos'].values, dtype=torch.float32).unsqueeze(1)
    y_pos = torch.tensor(node_features_df['y_pos'].values, dtype=torch.float32).unsqueeze(1)
    brick_type = torch.tensor(node_features_df['brick_type'].tolist(), dtype=torch.float32)
    orientation = torch.tensor(node_features_df['orientation'].tolist(), dtype=torch.float32)
    occupied = torch.tensor(node_features_df['occupied'].values, dtype=torch.float32).unsqueeze(1)
    insertion_point = torch.tensor(node_features_df['insertion_point'].values, dtype=torch.float32).unsqueeze(1)
    filler_module = torch.tensor(node_features_df['filler_module'].values, dtype=torch.float32).unsqueeze(1)
    eigenvector_centrality = torch.tensor(node_features_df['eigenvector_centrality'].values, dtype=torch.float32).unsqueeze(1)

    # Concatenate all node-level features into a matrix
    node_feature_matrix = torch.cat(
        [x_pos, y_pos, brick_type, orientation, occupied, insertion_point, filler_module, eigenvector_centrality], dim=1
    )

    # Assign the node features to the DGL graph
    dgl_graph.ndata['features'] = node_feature_matrix

    return dgl_graph

# Embed node features into the empty grid graph
empty_grid_dgl_graph = embed_node_features_in_dgl_graph(empty_grid_dgl_graph, combined_node_features[0])  # Assuming the empty grid has node features in index 0

# Now, embed node features into each pattern's DGL graph
for pattern_id, pattern_graph in all_pattern_dgl_graphs.items():
    node_features_df = combined_node_features[pattern_id]
    all_pattern_dgl_graphs[pattern_id] = embed_node_features_in_dgl_graph(pattern_graph, node_features_df)

# Print confirmation
print("Node features have been embedded in all DGL graphs.")

"""# Step 3: Recreating the Empty Grid Graph

## Step 3.1: Create the Empty Grid Graph (NetworkX)
"""

import networkx as nx

def create_empty_grid_graph(grid_width, grid_height):
    """Create an empty grid graph where each node represents the center of a grid cell, and edges connect adjacent cells."""
    G = nx.Graph()

    # Add nodes with their position as attributes
    for row in range(grid_height):
        for col in range(grid_width):
            node_id = row * grid_width + col
            x_center = col + 0.5
            y_center = row + 0.5
            G.add_node(node_id, pos=(x_center, y_center))

    # Add edges between adjacent nodes (horizontally and vertically)
    for row in range(grid_height):
        for col in range(grid_width):
            node_id = row * grid_width + col

            # Horizontal edge (right neighbor)
            if col < grid_width - 1:
                right_neighbor = row * grid_width + (col + 1)
                G.add_edge(node_id, right_neighbor, orientation='horizontal', length=1)

            # Vertical edge (bottom neighbor)
            if row < grid_height - 1:
                bottom_neighbor = (row + 1) * grid_width + col
                G.add_edge(node_id, bottom_neighbor, orientation='vertical', length=1)

    return G

# Create the empty grid graph
grid_width = 50  # Adjust the grid size as needed
grid_height = 50
empty_grid_graph = create_empty_grid_graph(grid_width, grid_height)

"""## Step 3.2: Create Empty Grid Node Features and Normalize Positions"""

import pandas as pd
from sklearn.preprocessing import MinMaxScaler

# Initialize the scaler for x and y positions
scaler = MinMaxScaler()

def create_empty_grid_node_features(empty_grid_graph):
    """Create default node features for the empty grid graph and normalize necessary fields."""
    node_features = []
    node_positions = nx.get_node_attributes(empty_grid_graph, 'pos')

    for node_id, pos in node_positions.items():
        node_feature = {
            'node_id': node_id,                     # Node ID
            'x_pos': pos[0],                        # Default x position
            'y_pos': pos[1],                        # Default y position
            'brick_type': [0, 0, 0, 0, 0],          # No brick assigned
            'orientation': [1, 0],                  # Default to horizontal
            'occupied': 0,                          # No node is occupied yet
            'insertion_point': 0,                   # No insertion point
            'filler_module': 0,                     # Not a filler module
            'eigenvector_centrality': 0             # Eigenvector centrality defaulted to 0
        }
        node_features.append(node_feature)

    # Convert to DataFrame for easier handling
    node_df = pd.DataFrame(node_features)

    # Normalize x_pos and y_pos (consistent with pattern graphs)
    node_df[['x_pos', 'y_pos']] = scaler.fit_transform(node_df[['x_pos', 'y_pos']])

    return node_df

# Generate the node features for the empty grid
empty_grid_node_features_df = create_empty_grid_node_features(empty_grid_graph)

"""## Step 3.3: Convert Empty Grid Graph and Features to DGL"""

import dgl
import torch

def convert_to_dgl_graph_with_normalization(G, node_features_df):
    """Convert a NetworkX graph and node features dataframe into a DGL graph."""
    # Convert the NetworkX graph to a DGL graph
    dgl_graph = dgl.from_networkx(G)

    # Prepare the node features in tensor format from the DataFrame
    x_pos = torch.tensor(node_features_df['x_pos'].values, dtype=torch.float32).unsqueeze(1)
    y_pos = torch.tensor(node_features_df['y_pos'].values, dtype=torch.float32).unsqueeze(1)
    brick_type = torch.tensor(node_features_df['brick_type'].tolist(), dtype=torch.float32)
    orientation = torch.tensor(node_features_df['orientation'].tolist(), dtype=torch.float32)
    occupied = torch.tensor(node_features_df['occupied'].values, dtype=torch.float32).unsqueeze(1)
    insertion_point = torch.tensor(node_features_df['insertion_point'].values, dtype=torch.float32).unsqueeze(1)
    filler_module = torch.tensor(node_features_df['filler_module'].values, dtype=torch.float32).unsqueeze(1)
    eigenvector_centrality = torch.tensor(node_features_df['eigenvector_centrality'].values, dtype=torch.float32).unsqueeze(1)

    # Concatenate all node-level features into a single matrix
    node_feature_matrix = torch.cat(
        [x_pos, y_pos, brick_type, orientation, occupied, insertion_point, filler_module, eigenvector_centrality], dim=1
    )

    # Assign node features to the DGL graph
    dgl_graph.ndata['features'] = node_feature_matrix

    return dgl_graph

# Convert the empty grid graph to DGL format
empty_grid_dgl_graph = convert_to_dgl_graph_with_normalization(empty_grid_graph, empty_grid_node_features_df)

# Print some information about the graph
print(f"Empty DGL Graph - Number of Nodes: {empty_grid_dgl_graph.num_nodes()}")
print(f"Empty DGL Graph - Number of Edges: {empty_grid_dgl_graph.num_edges()}")

# Print sample of empty grid features
print("Sample Node Features from the Empty Grid DGL Graph:")
print(node_features_df.head())

# Print sample of pattern graph features (from the combined node features in the dataset)
print("Sample Node Features from a Pattern Graph:")
print(combined_node_features[0].head())  # Assuming pattern ID 0 is available

"""# Step 4: Model Training Code

## 4.1 Define the Custom GraphSAGE Hybrid Model
"""

import torch
import torch.nn as nn
import dgl
import dgl.function as fn

# Custom GraphSAGE with branches for both node-level and graph-level tasks
class HybridGraphSAGE(nn.Module):
    def __init__(self, in_feats, hidden_feats, out_feats_node, out_feats_graph):
        super(HybridGraphSAGE, self).__init__()

        # Node-level GraphSAGE for local feature aggregation
        self.sage_node = dgl.nn.SAGEConv(in_feats, hidden_feats, aggregator_type='mean')

        # Graph-level GraphSAGE for global structure learning
        self.sage_graph = dgl.nn.SAGEConv(in_feats, hidden_feats, aggregator_type='mean')

        # Linear layers for node-level predictions (brick type, orientation)
        self.node_linear = nn.Linear(hidden_feats, out_feats_node)

        # Linear layers for graph-level predictions (modularity, completeness)
        self.graph_linear = nn.Linear(hidden_feats, out_feats_graph)

    def forward(self, g, features):
        # Node-level aggregation and predictions
        node_embeddings = self.sage_node(g, features)
        node_predictions = self.node_linear(node_embeddings)

        # Graph-level pooling and predictions
        g.ndata['h'] = node_embeddings
        graph_embeddings = dgl.mean_nodes(g, 'h')  # Pooling across nodes
        graph_predictions = self.graph_linear(graph_embeddings)

        return node_predictions, graph_predictions

print("HybridGraphSAGE model defined.")

"""## 4.2 Initialize Node and Graph Features with User Input

In Step 4.2, we simulate user inputs by initializing the empty grid with insertion points and brick types from each pattern. Here's the process:

    Node-Level Initialization:
        For each pattern, we set the brick_type, insertion_point, and occupied status for specific nodes based on the patternâ€™s data.

    Graph-Level Features:
        We extract graph-level features like primary_brick_type, selected_indices_count, and available_pattern_space to guide the model's understanding of the pattern.

    Pattern Processing:
        Each pattern is used to "pre-fill" the grid, simulating user input, allowing the model to learn how to predict the remaining bricks.

This setup prepares the model to train effectively by learning from initial inputs to complete the rest of the grid pattern.
"""

import torch

# Function to initialize the empty grid with user-like inputs based on the pattern's features
def initialize_grid_with_pattern_input(empty_grid_node_features_df, pattern_node_features, pattern_graph_features):
    """
    Initialize the grid with user-like inputs based on the pattern.
    """
    # Extract graph-level features from the pattern
    primary_brick_type = pattern_graph_features['primary_brick_type']
    selected_indices_count = pattern_graph_features['selected_indices_count']
    available_pattern_space = pattern_graph_features['available_pattern_space']

    # Fill the insertion points on the grid based on the pattern's node features
    for node_id, node_data in pattern_node_features.iterrows():
        if node_data['insertion_point'] == 1:
            empty_grid_node_features_df.at[node_id, 'brick_type'] = node_data['brick_type']
            empty_grid_node_features_df.at[node_id, 'occupied'] = 1
            empty_grid_node_features_df.at[node_id, 'insertion_point'] = 1

    return empty_grid_node_features_df, primary_brick_type, selected_indices_count, available_pattern_space

# Process one pattern
def process_pattern_for_training(pattern_id, empty_grid_node_features_df, combined_node_features, combined_graph_features):
    print(f"--- Processing Pattern ID: {pattern_id} ---")
    pattern_node_features = combined_node_features[pattern_id]
    pattern_graph_features = combined_graph_features[pattern_id]

    updated_node_features, primary_brick_type, selected_indices_count, available_pattern_space = initialize_grid_with_pattern_input(
        empty_grid_node_features_df.copy(), pattern_node_features, pattern_graph_features
    )

    return updated_node_features, primary_brick_type, selected_indices_count, available_pattern_space

for pattern_id, pattern_graph in all_pattern_dgl_graphs.items():
    updated_node_features, primary_brick_type, selected_indices_count, available_pattern_space = process_pattern_for_training(
        pattern_id, empty_grid_node_features_df, combined_node_features, combined_graph_features
    )

"""## 4.3 Define Labels for Node, Subgraph, and Graph-Level Prediction

extract the node labels (brick types) and graph labels (completeness, modularity) from our dataset.
"""

# Define labels for node-level prediction (brick type, etc.)
def define_node_labels(node_features_df):
    return node_features_df['brick_type'].values  # Example label: brick_type

# Define labels for subgraph-level prediction (completeness, etc.)
def define_subgraph_labels(subgraph_features_df):
    return subgraph_features_df['completeness'].values  # Example label: completeness

# Define labels for graph-level prediction (completeness score, etc.)
def define_graph_labels(graph_features_df):
    return graph_features_df['completeness_score'].values  # Example label: completeness score

# Generate labels
node_labels = {pattern_id: define_node_labels(combined_node_features[pattern_id]) for pattern_id in combined_node_features}
subgraph_labels = {pattern_id: define_subgraph_labels(combined_subgraph_features[pattern_id]) for pattern_id in combined_subgraph_features}
graph_labels = {pattern_id: define_graph_labels(combined_graph_features[pattern_id]) for pattern_id in combined_graph_features}

print("Node, subgraph, and graph-level labels defined.")

"""## Step 4.4: Data Preprocessing"""

import numpy as np
import pandas as pd
import torch

# Convert one-hot encoded labels to class indices
def convert_one_hot_to_class(labels):
    converted_labels = []
    for label in labels:
        if isinstance(label, (list, np.ndarray)):
            class_index = label.index(1) if 1 in label else 0
            converted_labels.append(class_index)
        else:
            converted_labels.append(int(label))
    return converted_labels

# Preprocess node labels
def preprocess_labels(node_labels):
    processed_node_labels = {}
    for pattern_id, labels in node_labels.items():
        processed_node_labels[pattern_id] = convert_one_hot_to_class(labels)
    return processed_node_labels

# Normalize node features
def normalize_node_features(node_features):
    processed_node_features = {}
    for pattern_id, features in node_features.items():
        df = pd.DataFrame(features)
        df[['x_pos', 'y_pos']] = (df[['x_pos', 'y_pos']] - df[['x_pos', 'y_pos']].min()) / (df[['x_pos', 'y_pos']].max() - df[['x_pos', 'y_pos']].min())
        processed_node_features[pattern_id] = df
    return processed_node_features

# Apply preprocessing
processed_node_labels = preprocess_labels(node_labels)
processed_node_features = normalize_node_features(combined_node_features)

print("Data preprocessing complete.")

"""## Step 4.5: Data Splitting"""

import random

# Split the dataset into training, validation, and test sets
def split_dataset(pattern_ids, train_ratio=0.7, val_ratio=0.15):
    random.shuffle(pattern_ids)
    train_size = int(len(pattern_ids) * train_ratio)
    val_size = int(len(pattern_ids) * val_ratio)

    train_ids = pattern_ids[:train_size]
    val_ids = pattern_ids[train_size:train_size + val_size]
    test_ids = pattern_ids[train_size + val_size:]

    return train_ids, val_ids, test_ids

all_pattern_ids = list(all_pattern_dgl_graphs.keys())
train_ids, val_ids, test_ids = split_dataset(all_pattern_ids)

print(f"Train set size: {len(train_ids)}")
print(f"Validation set size: {len(val_ids)}")
print(f"Test set size: {len(test_ids)}")

# Mask labels
def mask_labels(labels, mask_ids):
    masked_labels = {pattern_id: labels[pattern_id] for pattern_id in mask_ids if pattern_id in labels}
    return masked_labels

# Apply masks
train_node_labels = mask_labels(processed_node_labels, train_ids)
val_node_labels = mask_labels(processed_node_labels, val_ids)
test_node_labels = mask_labels(processed_node_labels, test_ids)

train_graph_labels = mask_labels(graph_labels, train_ids)
val_graph_labels = mask_labels(graph_labels, val_ids)
test_graph_labels = mask_labels(graph_labels, test_ids)

"""## Step 4.6: Set up Hyperparameters"""

# Set hyperparameters for the model training
in_feats = 14  # Number of input features
hidden_feats = 64  # Number of hidden units in SAGEConv layer
out_feats_node = 5  # Number of classes for node-level classification
out_feats_graph = 1  # Output size for graph-level prediction
num_epochs = 150  # Number of epochs for training
learning_rate = 0.001  # Learning rate for the optimizer

"""## Step 4.7: Train the Model with Defined Optimizers"""

# Function to ensure that all necessary columns are retained
def flatten_features(df):
    """
    Ensures all features in the DataFrame are numeric by flattening lists/arrays or converting them.
    """
    def flatten_value(value):
        if isinstance(value, (list, np.ndarray)):  # If the value is a list or array
            return np.mean(value)  # Take the average or some other numeric representation
        return value  # Return the value if it's already numeric (float, int)

    flattened_df = df.applymap(flatten_value)  # Flatten all features

    # Check if we have all the necessary columns (adjust these to your actual column names)
    required_columns = ['x_pos', 'y_pos', 'brick_type', 'orientation', 'occupied', 'insertion_point', 'filler_module', 'eigenvector_centrality']
    if len(flattened_df.columns) != len(required_columns):
        print(f"Warning: Expected {len(required_columns)} features but got {len(flattened_df.columns)}.")

    return flattened_df

import torch
import torch.nn as nn

# Initialize the Hybrid GraphSAGE model (referencing Step 4.1)
model = HybridGraphSAGE(in_feats, hidden_feats, out_feats_node, out_feats_graph)

# Utility function to flatten or convert non-numeric features
def flatten_features(df):
    """
    Ensures all features in the DataFrame are numeric by flattening lists/arrays or converting them.
    """
    def flatten_value(value):
        if isinstance(value, (list, np.ndarray)):  # If the value is a list or array
            return np.mean(value)  # Take the average or some other numeric representation
        return value  # Return the value if it's already numeric (float, int)

    return df.applymap(flatten_value)  # Apply the flatten_value function to all DataFrame elements

# Training function with validation and debugging
def train_hybrid_graphsage_with_validation(model, graphs, node_features, node_labels, graph_labels, train_ids, val_ids, num_epochs, lr):
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    loss_fn = nn.CrossEntropyLoss()  # For classification tasks
    graph_loss_fn = nn.MSELoss()  # For continuous graph-level predictions

    train_losses = []
    val_losses = []
    val_accuracies = []

    for epoch in range(num_epochs):
        model.train()
        epoch_train_loss = 0

        # Training loop
        for graph_id in train_ids:
            try:
                dgl_graph = graphs[graph_id]

                # Flatten non-numeric features to ensure compatibility with PyTorch tensor
                node_features_prepared = flatten_features(node_features[graph_id])
                node_feats = torch.tensor(node_features_prepared.values, dtype=torch.float32)

                # Use preprocessed node labels
                node_labels_tensor = torch.tensor(node_labels[graph_id], dtype=torch.long)
                graph_labels_tensor = torch.tensor(graph_labels[graph_id], dtype=torch.float32)

                # Forward pass
                node_preds, graph_preds = model(dgl_graph, node_feats)

                # Compute losses
                node_loss = loss_fn(node_preds, node_labels_tensor)
                graph_loss = graph_loss_fn(graph_preds, graph_labels_tensor.unsqueeze(0))  # Add batch dimension
                total_loss = node_loss + graph_loss

                # Backpropagation
                optimizer.zero_grad()
                total_loss.backward()
                optimizer.step()

                epoch_train_loss += total_loss.item()

            except KeyError as e:
                print(f"Error processing graph {graph_id}: {e}")
                continue

        # Validation loop (no gradient computation)
        model.eval()
        epoch_val_loss = 0
        correct_preds = 0
        total_preds = 0
        with torch.no_grad():
            for graph_id in val_ids:
                try:
                    dgl_graph = graphs[graph_id]

                    # Flatten non-numeric features to ensure compatibility with PyTorch tensor
                    node_features_prepared = flatten_features(node_features[graph_id])
                    node_feats = torch.tensor(node_features_prepared.values, dtype=torch.float32)

                    # Use preprocessed node labels
                    node_labels_tensor = torch.tensor(node_labels[graph_id], dtype=torch.long)
                    graph_labels_tensor = torch.tensor(graph_labels[graph_id], dtype=torch.float32)

                    # Forward pass (validation)
                    node_preds, graph_preds = model(dgl_graph, node_feats)

                    # Compute validation losses
                    node_loss = loss_fn(node_preds, node_labels_tensor)
                    graph_loss = graph_loss_fn(graph_preds, graph_labels_tensor.unsqueeze(0))  # Add batch dimension
                    total_loss = node_loss + graph_loss

                    epoch_val_loss += total_loss.item()

                    # Calculate validation accuracy
                    predicted_labels = torch.argmax(node_preds, dim=1)
                    correct_preds += (predicted_labels == node_labels_tensor).sum().item()
                    total_preds += node_labels_tensor.size(0)

                except KeyError as e:
                    print(f"Error processing validation graph {graph_id}: {e}")
                    continue

        # Log epoch losses and accuracy
        train_losses.append(epoch_train_loss / len(train_ids))
        val_losses.append(epoch_val_loss / len(val_ids))
        val_accuracies.append(correct_preds / total_preds if total_preds > 0 else 0)
        print(f"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_losses[-1]}, Validation Loss: {val_losses[-1]}, Validation Accuracy: {val_accuracies[-1]}")

    return train_losses, val_losses, val_accuracies

# Train the model with debugging
train_losses, val_losses, val_accuracies = train_hybrid_graphsage_with_validation(
    model, all_pattern_dgl_graphs, processed_node_features, processed_node_labels, graph_labels,
    train_ids, val_ids, num_epochs=num_epochs, lr=learning_rate
)

"""## Step 4.8: Plot Training and Validation Graphs"""

import matplotlib.pyplot as plt

# Plot the training and validation losses
plt.figure(figsize=(12, 5))

# Plot losses
plt.subplot(1, 2, 1)
plt.plot(train_losses, label='Training Loss', color='blue')
plt.plot(val_losses, label='Validation Loss', color='red')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Training & Validation Loss')
plt.legend()

# Plot validation accuracy
plt.subplot(1, 2, 2)
plt.plot(val_accuracies, label='Validation Accuracy', color='green')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.title('Validation Accuracy')
plt.legend()

plt.tight_layout()
plt.show()

"""## Step 4.9: Test the Model on Unseen Data and Visualize Brick Type Labels


"""

# Test the trained model on unseen graphs (from the test set)
def test_model_on_test_set(model, graphs, node_features, node_labels, graph_labels, test_ids):
    model.eval()
    test_loss = 0
    loss_fn = nn.CrossEntropyLoss()
    graph_loss_fn = nn.MSELoss()

    correct_preds = 0
    total_preds = 0

    with torch.no_grad():
        for graph_id in test_ids:
            dgl_graph = graphs[graph_id]
            node_feats = torch.tensor(node_features[graph_id].values, dtype=torch.float32)
            node_labels_tensor = torch.tensor(node_labels[graph_id], dtype=torch.long)
            graph_labels_tensor = torch.tensor(graph_labels[graph_id], dtype=torch.float32)

            node_preds, graph_preds = model(dgl_graph, node_feats)

            node_loss = loss_fn(node_preds, node_labels_tensor)
            graph_loss = graph_loss_fn(graph_preds, graph_labels_tensor.unsqueeze(0))
            total_loss = node_loss + graph_loss

            test_loss += total_loss.item()

            predicted_labels = torch.argmax(node_preds, dim=1)
            correct_preds += (predicted_labels == node_labels_tensor).sum().item()
            total_preds += node_labels_tensor.size(0)

    accuracy = correct_preds / total_preds
    print(f"Test Loss: {test_loss / len(test_ids)}, Test Accuracy: {accuracy}")

    return accuracy

# Test the model
test_accuracy = test_model_on_test_set(model, all_pattern_dgl_graphs, processed_node_features, test_node_labels, test_graph_labels, test_ids)

# Visualize predicted brick types for a sample graph from the test set
def visualize_test_results(graph_id, predicted_labels, grid_height, grid_width):
    fig, ax = plt.subplots(figsize=(6, 6))

    grid_image = np.ones((grid_height, grid_width, 3)) * 255  # White background
    label_color_map = {
        0: [0, 0, 255],  # Blue (2x4 brick)
        1: [255, 0, 0],  # Red (1x1 brick)
        2: [0, 255, 0],  # Green (1x2 brick)
        3: [255, 255, 0],  # Yellow (1x3 brick)
        4: [255, 0, 255]  # Magenta (1x4 brick)
    }

    for idx, label in enumerate(predicted_labels):
        row = idx // grid_width
        col = idx % grid_width
        grid_image[row, col] = label_color_map[label]

    ax.imshow(grid_image.astype(np.uint8), extent=(0, grid_width, grid_height, 0))
    ax.set_title(f"Predicted Brick Types for Graph {graph_id}")
    ax.axis('off')
    plt.show()

# Visualize results for a sample graph
sample_graph_id = test_ids[0]
predicted_labels = torch.argmax(model(all_pattern_dgl_graphs[sample_graph_id], torch.tensor(processed_node_features[sample_graph_id].values, dtype=torch.float32))[0], dim=1).numpy()
visualize_test_results(sample_graph_id, predicted_labels, grid_height=50, grid_width=50)

"""# Step 5: Saving the Model"""

# Assuming you have the same model and learning rate as before
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

# Now, save the optimizer's state
optimizer_save_path = '/content/drive/MyDrive/BrickLayerX_2000/optimizer_state.pth'
torch.save(optimizer.state_dict(), optimizer_save_path)

print(f"Optimizer state saved to {optimizer_save_path}")

# Define the path where you want to save the model
save_path = '/content/drive/MyDrive/BrickLayerX_2000/trained_hybrid_graphsage_model.pth'

# Save the trained model to Google Drive
torch.save(model.state_dict(), save_path)

# Optionally, save the optimizer's state as well if you want to resume training later
optimizer_save_path = '/content/drive/MyDrive/BrickLayerX_2000/optimizer_state.pth'
torch.save(optimizer.state_dict(), optimizer_save_path)

print(f"Model and optimizer saved to {save_path} and {optimizer_save_path}")

"""# Step 6: Load the Saved Model

## 1. Reimport Libraries
"""

import torch
import torch.nn as nn
import dgl
import dgl.function as fn

"""## 2. Redefine the HybridGraphSAGE Model"""

# Custom GraphSAGE with branches for both node-level and graph-level tasks
class HybridGraphSAGE(nn.Module):
    def __init__(self, in_feats, hidden_feats, out_feats_node, out_feats_graph):
        super(HybridGraphSAGE, self).__init__()

        # Node-level GraphSAGE for local feature aggregation
        self.sage_node = dgl.nn.SAGEConv(in_feats, hidden_feats, aggregator_type='mean')

        # Graph-level GraphSAGE for global structure learning
        self.sage_graph = dgl.nn.SAGEConv(in_feats, hidden_feats, aggregator_type='mean')

        # Linear layers for node-level predictions (brick type, orientation)
        self.node_linear = nn.Linear(hidden_feats, out_feats_node)

        # Linear layers for graph-level predictions (modularity, completeness)
        self.graph_linear = nn.Linear(hidden_feats, out_feats_graph)

    def forward(self, g, features):
        # Node-level aggregation and predictions
        node_embeddings = self.sage_node(g, features)
        node_predictions = self.node_linear(node_embeddings)

        # Graph-level pooling and predictions
        g.ndata['h'] = node_embeddings
        graph_embeddings = dgl.mean_nodes(g, 'h')  # Pooling across nodes
        graph_predictions = self.graph_linear(graph_embeddings)

        return node_predictions, graph_predictions

        # These are the same values you used during model training
in_feats = 14  # Number of input features (e.g., x_pos, y_pos, brick_type, etc.)
hidden_feats = 64  # Number of hidden units in the GraphSAGE layer
out_feats_node = 5  # Number of output classes for node-level classification (e.g., brick types)
out_feats_graph = 1  # Number of output features for graph-level prediction (e.g., modularity/completeness)

"""## 3. Load the Saved Model"""

# Define the HybridGraphSAGE model (same as the one used during training)
model = HybridGraphSAGE(in_feats, hidden_feats, out_feats_node, out_feats_graph)

# Define the path to the saved model
model_save_path = '/content/drive/MyDrive/Colab Notebooks/Thesis/BrickLayerX_2000/trained_hybrid_graphsage_model.pth'

# Load the saved model weights
model.load_state_dict(torch.load(model_save_path))

# Set the model to evaluation mode
model.eval()

print("Model loaded successfully.")